# ğŸš€ Automated Multi-Source Sales Analytics Pipeline

A modern data engineering project demonstrating a hybrid-cloud ETL pipeline. This project orchestrates data movement from a **Local PostgreSQL** instance to **Databricks Community Edition** using **Apache Airflow**, with a mirrored CI/CD workflow between **GitLab** and **GitHub**.

## ğŸ—ï¸ Architecture
1.  **Source:** Local PostgreSQL (Operational DB).
2.  **Orchestrator:** Apache Airflow (DAG management).
3.  **Compute/Transform:** Databricks (PySpark / Spark SQL).
4.  **Storage:** Delta Lake (Gold Layer summary tables).
5.  **DevOps:** GitLab (Primary Dev) mirrored to GitHub (Public Portfolio).

## ğŸ› ï¸ Tech Stack
* **Orchestration:** Apache Airflow
* **Data Processing:** PySpark / Databricks
* **Database:** PostgreSQL
* **Version Control:** GitLab & GitHub
* **Language:** Python 3.x

## ğŸ“ Project Structure
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ sales_main_dag.py          # The Airflow orchestration file
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ helpers.py             # Reusable Python functions (e.g., logging)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ silver_to_gold_sales.py    # Databricks PySpark transformation code
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ postgres_init.sql          # Script to set up local DB
â”‚   â””â”€â”€ analytical_queries.sql     # Sample queries for the final Delta Table
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ airflow_connections.yaml   # (Reference only) Documentation for connections
â”‚   â””â”€â”€ databricks_job_config.json # JSON spec for the Databricks job
â”œâ”€â”€ .gitignore                     # To hide .env, __pycache__, and DS_Store
â”œâ”€â”€ README.md                      # Project documentation
â””â”€â”€ requirements.txt               # List of python libraries needed