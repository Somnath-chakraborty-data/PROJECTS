# ğŸš€ Automated Multi-Source Sales Analytics Pipeline

A modern data engineering project demonstrating a hybrid-cloud ETL pipeline. This project orchestrates data movement from a **Local PostgreSQL** instance to **Databricks Community Edition** using **Apache Airflow**, with a mirrored CI/CD workflow between **GitLab** and **GitHub**.

## ğŸ—ï¸ Architecture
1.  **Source:** Local PostgreSQL (Operational DB).
2.  **Orchestrator:** Apache Airflow (DAG management).
3.  **Compute/Transform:** Databricks (PySpark / Spark SQL).
4.  **Storage:** Delta Lake (Gold Layer summary tables).
5.  **DevOps:** GitLab (Primary Dev) mirrored to GitHub (Public Portfolio).

## ğŸ› ï¸ Tech Stack
* **Orchestration:** Apache Airflow
* **Data Processing:** PySpark / Databricks
* **Database:** PostgreSQL
* **Version Control:** GitLab & GitHub
* **Language:** Python 3.x

## ğŸ“ Project Structure
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ sales_orchestrator.py      # Airflow DAG defining the workflow
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ transform_sales.py         # PySpark transformation logic
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ init_db.sql                # Postgres schema setup
â”œâ”€â”€ .gitignore                     # Prevents secrets/env leakage
â””â”€â”€ README.md
